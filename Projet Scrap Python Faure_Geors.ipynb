{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/hfg/anaconda3/lib/python3.8/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hfg/anaconda3/lib/python3.8/site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: joblib in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (0.17.0)\n",
      "Requirement already satisfied: tqdm in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.50.2)\n",
      "Requirement already satisfied: regex in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2020.10.15)\n",
      "Requirement already satisfied: click in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: textblob_fr in /home/hfg/anaconda3/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: textblob>=0.8.0 in /home/hfg/anaconda3/lib/python3.8/site-packages (from textblob_fr) (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/hfg/anaconda3/lib/python3.8/site-packages (from textblob>=0.8.0->textblob_fr) (3.5)\n",
      "Requirement already satisfied: regex in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (2020.10.15)\n",
      "Requirement already satisfied: joblib in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (0.17.0)\n",
      "Requirement already satisfied: click in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/hfg/anaconda3/lib/python3.8/site-packages (from nltk>=3.1->textblob>=0.8.0->textblob_fr) (4.50.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hfg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/hfg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "                                                    #Methodes du projet\n",
    "!pip install textblob\n",
    "!pip install textblob_fr    \n",
    "from requests import get\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as pyp\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from heapq import nlargest\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer    \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "sw = stopwords.words('french') \n",
    "sw.append('a')\n",
    "sw.append(\"d'\")\n",
    "sw.append(\"n'\")\n",
    "sw.append(\"l'\")#Ces mots ne sont pas compté alors qu'ils sont récurrents, ils gênent l'attribution de poids aux phrases des articles\n",
    "punctuation = string.punctuation + '’«»—.\\n'#Ponctuation française trouvé dans les articles de CNEWS\n",
    "main_url = 'https://www.cnews.fr/'\n",
    "test_url = '/culture/2021-01-05/pas-de-date-de-reprise-pour-le-secteur-culturel-annonce-roselyne-bachelot-1033263'\n",
    "\n",
    "def Check_Web_Connection(actual_url):\n",
    "    response = get(actual_url)\n",
    "    return response\n",
    "\n",
    "def Get_Web_Soup(response):\n",
    "    html_soup = BeautifulSoup(response.text,\"html.parser\")#Récupère l'ensemble du site web \n",
    "    return html_soup\n",
    "\n",
    "def Get_URL_main_articles(soup):#Récupération des articles mis en avant par le site\n",
    "    body = soup.find(id='main-content')\n",
    "    temporary_url_array = []\n",
    "    main_art = body.find_all('div',class_=\"dm-block dm-block-bloc_1_news\")\n",
    "    for elem in main_art:\n",
    "        url = elem.find('a',href=True)\n",
    "        temporary_url_array.append(url['href'])\n",
    "    url_array = [temporary_url_array[0],temporary_url_array[1],temporary_url_array[2],temporary_url_array[3]]\n",
    "    #Sur CNews, les derniers gros titres correspondent souvent à de la culture et non à des informations sur l'actualité\n",
    "    return(url_array)\n",
    "\n",
    "def Get_URL_sub_articles(soup):#Récupération des autres articles\n",
    "    body = soup.find(id='main-content')\n",
    "    temporary_url_array = []\n",
    "    url_array = []\n",
    "    main_art = body.find_all('div',class_=\"dm-block dm-block-bloc_3_news\")\n",
    "    for elem in main_art:\n",
    "        url = elem.find_all('a',href=True)\n",
    "        for i in url:\n",
    "            temporary_url_array.append(i['href'])\n",
    "    for i in range(15): #Ce nombre d'URL peut être modifié en fonction du nombre d'article qu'on veut résumer\n",
    "        url_array.append(temporary_url_array[i])\n",
    "    return(url_array)\n",
    "\n",
    "def Get_text_from_article(actual):#Récupère l'ensemble des morceaux de texte contenu dans l'article, contient également les textes assosiés aux tweets cités ainsi que les URL que je n'ai pas réussi à retirer \n",
    "    actual_url= main_url + actual[1:]\n",
    "    text = []\n",
    "    response = Check_Web_Connection(actual_url)\n",
    "    soup = Get_Web_Soup(response)#On récupère l'ensemble du site avant d'y appliquer des recherches via soup.find\n",
    "    body= soup.find(id=\"wrapper-publicite\")\n",
    "    titre = body.find(\"h1\",class_=\"article-title\")\n",
    "    if titre != None: #Il y a quelques problèmes avec l'uniformité des pages du site web, le titre est récupéré sur certains articles mais pas sur tous\n",
    "        text.append(titre.text + \".\")\n",
    "        text.append(\" \")\n",
    "    intro = body.find(\"p\", class_=\"dm_article-chapeau\")\n",
    "    text.append(intro.text)\n",
    "    text.append(\" \")\n",
    "    content_1 = body.find_all(\"p\",class_=\"dm_article-paragraph\")\n",
    "    content_2 = body.find_all('p',class_= None)#Certains morceaux de texte ne sont associés à aucune classe\n",
    "    for elem in content_1:\n",
    "        text.append(elem.text)\n",
    "        text.append(' ')\n",
    "    for elem in content_2:\n",
    "        text.append(elem.text)\n",
    "        text.append(' ')\n",
    "    article = ''.join(text)\n",
    "    return article\n",
    "\n",
    "def Print_all_articles():#Affiche tout les articles du site\n",
    "    #ON commence par afficher les grands articles du site\n",
    "    response = Check_Web_Connection(main_url)\n",
    "    soup = Get_Web_Soup(response)\n",
    "    url_prince = Get_URL_main_articles(soup)\n",
    "    url_sub = Get_URL_sub_articles(soup)\n",
    "    print(\"Voici les articles correspondant aux gros titres de Cnews\\n\")\n",
    "    for elem in url_prince:  \n",
    "        print(Get_text_from_article(elem))\n",
    "        print(\"\\n\"*3)\n",
    "    print(\"Voici tout les autres articles proposé par Cnews\")\n",
    "    for elem in url_sub:  \n",
    "        print(Get_text_from_article(elem))\n",
    "        print(\"\\n\"*3)\n",
    "        \n",
    "        \n",
    "def Get_num(article):#Obtiens un dictionnaire indiquant le nombre d'ittération de chaque mot de l'article\n",
    "    num = {}\n",
    "    tokenizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''') #J'utilise RegexpTokenizer pour les mots car en utilisant word_tokenize, j'avais certains problèmes causés par la langue française / les textes dans les tweets\n",
    "    tok_art = tokenizer.tokenize(article)\n",
    "    for word in tok_art:\n",
    "        if word.lower() not in sw:\n",
    "            if word.lower() not in punctuation: \n",
    "                if word in num.keys():\n",
    "                    num[word] = num[word] + 1 #On compte le nombre de fois qu'un mot apparait dans le texte \n",
    "                else:\n",
    "                    num[word] = 1\n",
    "    return num\n",
    "\n",
    "\n",
    "def Get_value_sentence(sentence, num):#Retourne le poids d'une phrase de l'article\n",
    "    tokenizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "    sent_tok = tokenizer.tokenize(sentence)\n",
    "    count = 0\n",
    "    for word in sent_tok:#On calcule le poids de chaque mots de la phrase grâce au dictionnaire de fréquence qu'on à déterminé précédemment\n",
    "        if word.lower() not in sw:\n",
    "            if word.lower() not in punctuation:\n",
    "                count = count + num[word]\n",
    "    count = count / len(sent_tok)#On effectue une moyenne du poids de chaque mots de façon à ne pas avantager les phrases très longues qui peuvent s'avérer ne pas être essentielles\n",
    "    return count\n",
    "\n",
    "def Get_weight_sentences(num,article):#Retourne le poids de chaque phrase de l'article\n",
    "    tok_art = sent_tokenize(article, language='french')#on tokenize l'articles en phrases de façon à pouvoir évaluer le poids de chaque phrase\n",
    "    weight = {}    \n",
    "    for sentences in tok_art:\n",
    "        weight[sentences] = Get_value_sentence(sentences,num)\n",
    "    max_val = weight.values()\n",
    "    weight[tok_art[0]] = max(max_val)#Permet d'avoir forcement le titre de l'article dans son résumé ce qui rend la compréhension de l'article plus simple\n",
    "    return weight  \n",
    "\n",
    "def Summary_article(article):\n",
    "    num = Get_num(article)\n",
    "    weight = Get_weight_sentences(num,article)\n",
    "    length = int(len(weight)*0.25)#Ce nombre peut être modifié selon la taille de résumé que l'on désire, actuellement, on récupère les 25% des phrases de l'articles qui ont le plus grand poids\n",
    "    summary_array = nlargest(length,weight,key=weight.get)#Renvoie un tableau contenant un pourcentage des phrases ayant le plus grand poids dans l'article\n",
    "    summary= ''.join(summary_array)\n",
    "    return summary\n",
    "\n",
    "def Sentiment_article(article): #Permet d'obtenir le sentiment dégagé d'un article: 0 étant plutot neutre, 1 positif et -1 négatif\n",
    "    tok_art = sent_tokenize(article, language='french')\n",
    "    sentiment = 0\n",
    "    for sentences in tok_art:\n",
    "        sentiment = sentiment + TextBlob(sentences,pos_tagger = PatternTagger(), analyzer = PatternAnalyzer()).sentiment[0]\n",
    "    sentiment = sentiment / len(tok_art)\n",
    "    return sentiment\n",
    "\n",
    "def Print_sentiment(sentiment_array): #Permet d'afficher la polarité de tout les articles\n",
    "    pyp.plot(sentiment_array)\n",
    "    axes = pyp.gca()\n",
    "    axes.set_ylim(-1,1)\n",
    "    pyp.xlabel(\"Positionnement de l'article dans le fil d'actualité\")\n",
    "    pyp.ylabel(\"Polarité de l'article (1 = positif, -1 négatif , 0 neutre)\")\n",
    "    pyp.suptitle(\"Polarité des articles actuels de CNEWS\")\n",
    "\n",
    "def Print_detail_article():\n",
    "    reponse = Check_Web_Connection(main_url)\n",
    "    main_soup = Get_Web_Soup(reponse)\n",
    "    text = Get_text_from_article(test_url)\n",
    "    print(\"L'article \\n\")\n",
    "    print(text + \"\\n\"*4)\n",
    "    num = Get_num(text)\n",
    "    sorted_num = {}\n",
    "    sorted_keys = sorted(num,key=num.get, reverse = True)\n",
    "    for elem in sorted_keys:\n",
    "        sorted_num[elem]=num[elem]\n",
    "    num_list = sorted_num.items()\n",
    "    temp_x,temp_y = zip(*num_list)\n",
    "    x=temp_x[:7]\n",
    "    y= temp_y[:7]\n",
    "    pyp.plot(x,y)\n",
    "    pyp.ylabel(\"Nombre d'ittération\")\n",
    "    pyp.xlabel(\"Mots les plus employés dans l'article\")\n",
    "    pyp.suptitle(\"Répartition des mots dans l'article\")\n",
    "    print(\"\\n\"*4)\n",
    "    print(\"Le poids de chaque phrase de l'article\")\n",
    "    print(Get_weight_sentences(num,text))\n",
    "    print(\"\\n\"*4)\n",
    "    print(\"Le résumé de l'article : \" + Summary_article(text))\n",
    "    print(\"\\n\"*4 + \"La polarité de l'article (-1 négatif, 0 neutre, 1 positif)\")\n",
    "    print(Sentiment_article(text))\n",
    "    \n",
    "def Synthese_sentiment():\n",
    "    reponse = Check_Web_Connection(main_url)\n",
    "    main_soup = Get_Web_Soup(reponse)\n",
    "    url_princ = Get_URL_main_articles(main_soup)\n",
    "    url_sub = Get_URL_sub_articles(main_soup)\n",
    "    sent_array = []\n",
    "    for articles in url_princ:\n",
    "        text = Get_text_from_article(articles)\n",
    "        sent_array.append(Sentiment_article(text))\n",
    "    \n",
    "    for articles in url_sub:\n",
    "        text = Get_text_from_article(articles)\n",
    "        sent_array.append(Sentiment_article(text))\n",
    "    Print_sentiment(sent_array)\n",
    "    \n",
    "def Resume_articles():\n",
    "    reponse = Check_Web_Connection(main_url)\n",
    "    main_soup = Get_Web_Soup(reponse)\n",
    "    url_princ = Get_URL_main_articles(main_soup)\n",
    "    url_sub = Get_URL_sub_articles(main_soup)\n",
    "    sent_array = []\n",
    "    for articles in url_princ:\n",
    "        text = Get_text_from_article(articles)\n",
    "        print(Summary_article(text))\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    for articles in url_sub:\n",
    "        text = Get_text_from_article(articles)\n",
    "        print(Summary_article(text))\n",
    "        print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impression de tous les articles (en fonction du nombre choisi dans la méthode Get_Url_Sub_articles)\n",
    "Print_all_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Résumé des articles\n",
    "Resume_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph montrant les sentiments dégagés des articles Cnews\n",
    "Synthese_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Méthode montrant en détail le procédé de synthèse d'un article (le graphique se positionne en bas naturellement)\n",
    "Print_detail_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
